TODO:
1、hotest的时候，除了用taobao api更新，同时也更新评论和销量
2、评论抓取的优先级调整，好的商品优先抓




公网的爬虫sdl-guang-crawl1
 cron/crawler --> /etc/cron.d/crawler

内网的爬虫crawl[1-3].jcndev.com
 /etc/rc.local
 /etc/cron.d/crawler

内网的日志检查：
    /var/spool/cron/crontabs/root

------------------------

爬虫

1、内网 crawl1.jcndev.com 拨号服务器，抓取淘宝页面列表

淘宝列表页 --> tb_shop_item/tb_crawl_shop_items

crontab : /home/chris/bin/exectimeout -t 10 nc 127.0.0.1 3306 保持3306链接
daemon : sshproxy
/usr/bin/python /usr/local/bin/sshforward --lport 9089 --rport 9089 --rhost 192.168.10.97 --pidfile /home/chris/run/ssh4redis.pid --daemon --use_logfile
/usr/bin/python /usr/local/bin/sshforward --lport 3306 --rport 3306 --rhost 192.168.32.10 --pidfile /home/chris/run/ssh4mysql.pid --daemon --use_logfile

/usr/local/bin/crawl_redial
-->>  /usr/bin/python /usr/local/bin/crawl_shop.py --stderr --color --verbose debug --commit --interval 1500 --use_logfile --recent --where shop.id>=0 and shop.id!=387 and shop.id!=388

/etc/rc.local -->
/usr/sbin/openvpn --config /etc/openvpn/server.conf & >/dev/null 2>&1

/sbin/route add -net 192.168.2.0 netmask 255.255.255.0 gw 192.168.1.1
/sbin/route del default gw 218.241.226.177
/sbin/route del default gw 192.168.1.1
/sbin/route add default dev ppp0
/sbin/route add 211.100.28.148 gw 218.241.226.177

/sbin/ip rule add from 218.241.226.183 table cnc
/sbin/ip route add default via 218.241.226.177 table cnc

nohup /usr/local/bin/crawl_redial &
-------->

2、内网 crawl2.jcndev.com 爬评论的服务器

item --> redis 192.168.10.97:9089 （guang-squid1） redis-web.conf 8g
   backup redis 192.168.10.80:9089 (guang-redis1) 32g

   echo "info" | redis-cli -h 192.168.10.97 -p 9089

cd /space/code/taobao-crawler; bin/repeat_crawl_all_comments.sh
cd /space/code/taobao-crawler; bin/daily_crawl_comments

3、内网 crawl3.jcndev.com， 公网 guangcrawldb1

4、guangcrawl1

b2c抓取
0 */8 * * * (cd /space/crawler; /space/crawler/bin/quick_update_b2c.sh > /dev/null 2>&1)
0 10 * * * (cd /space/crawler; /space/crawler/bin/monb2c.sh)

更新状态 --> 调用taobao api
30 */1 * * * (cd /space/crawler; /usr/local/bin/quick_update_taobao_status.py --use_logfile --verbose info --dbhost 192.168.32.10)

标记全部抓取
0 0,12,19 * * * (cd /space/crawler; /usr/local/bin/mark_shop2crawl.py --use_logfile --verbose info --dbhost 192.168.32.10)

定时启动抓取api
*/5 * * * * (cd /space/crawler; /space/crawler/bin/crawl_monitor.sh > /dev/null 2>&1)

更新淘宝店的星级
0 0 2 * * (cd /space/crawler; /usr/local/bin/update_shop_level.py --use_logfile --verbose info --all --dbhost 192.168.32.10)

转换淘宝客链接
0 */2 * * * (cd /space/crawler; /usr/local/bin/process_taobaoke.py --dbhost 192.168.32.10 --use_logfile --verbose info)

下载淘宝客报表
0 4 * * * (cd /space/crawler; /usr/local/bin/get_taobao_report.py --use_logfile --verbose info --interval 15 --dbhost 192.168.32.10)

爬详情页html和图片
sudo /usr/bin/python /usr/local/bin/crawl.py --verbose info --stderr --color --use_logfile --pending --interval 2000 --parallel --max_cpu 2 --dnscache_retry_per_exception 10000 --removetmp --dbhost 192.168.32.10

TODO：
  定期杀死没反应的 process_taobaoke/quick_update_taobao_status/...

